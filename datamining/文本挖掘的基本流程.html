文本挖掘的基本流程
来源 http://blog.csdn.net/u011274209/article/details/51896757

一、获取文本

  我们获取网络文本，主要是获取网页HTML的形式。我们要把网络中的文本获取文本数据库(数据集)。编写爬虫（Spider）程序,抓取到网络中的信息。可以用广度优先和深度优先；根据用户的需求，爬虫可以有垂直爬虫和通用爬虫之分，垂直爬取主要是在相关站点爬取或者爬取相关主题的文本 ，而通用爬虫则一般对此不加限制。爬虫可以自己写，当然现在网络上已经存在很多开源的爬虫系统（比如Python的Scrapy和pyspider）。

二、对文本进行预处理

  网页中存在很多不必要的信息，比如说一些广告，导航栏，html、js代码，注释等等，我们并不感兴趣的信息,可以delete掉。如果是需要正文提取，可以利用标签用途、标签密度判定、数据挖掘思想、视觉网页块分析技术等等策略抽取出正文。

三、文本流的语言学处理

1、分词

  经过上面的步骤，我们会得到比较干净的素材。文本中起到关键作用的是一些词，甚至主要词就能起到决定文本取向。比如说一篇文章讲的是体育还是娱乐，肯定是对文章中的中心词进行分析得到的结果。 
  在找出中心词之前，我们首先得在每个文本中得到所有词吧。这里就会用到一个分词系统或者说分词工具。现在针对中文分词，出现了很多分词的算法，有最大匹配法、最优匹配法、机械匹配法、逆向匹配法、双向匹配法等等（可以参考各类文献）。我们经常用到的中科院的分词工具ICTCLAS，该算法经过众多科学家的认定是当今中文分词中很好的，并且支持用户自定义词典，加入词典,；对新词，人名，地名等的发现也具有良好的效果。

2、词性标注

  同时也可以使用词性标注。通过很多分词工具分出来的出会出现一个词，外加该词的词性。比如说啊是语气助词。 
  利用python的jieba分词，比如words = pseg.cut(“我爱北京天安门”)进行词性标注，得到的结果是 ：我 r 爱 v 北京 ns 天安门 ns

3、去除停用词

  经过上面的步骤，我们已经把所有的词进行了分类。但是这些所有的词，并不都是我们所需要的，比如说句号（。）显然，句号对意思的表达没有什么效果。还有”是”、”的”等词，也没有什么效果。因为这些词在所有的文章中都大量存在，并不能反应出文本的意思，可以处理掉。当然针对不同的应用还有很多其他词性也是可以去掉的，比如形容词等。

四、文本流的数学处理

  我们希望，获取到的词汇，最能保留它们的信息，同时反映了它们的地位。有些词汇，它们往往也不能决定文章的内容。还有一个原因就是，如果对所有词语都保留，维度会特别高，矩阵将会变得特别稀疏，严重影响到挖掘结果。针对特征选择也有很多种不同的方式，但是改进后的TFIDF往往起到的效果是最好的。

1、TFIDF

  TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TFIDF实际上是：TFIDF，TF词频（Term Frequency），IDF反文档频率（Inverse DocumentFrequency）。TF表示词条，在文档d中出现的频率。IDF的主要思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。 
tfi,j=ni,j∑knk,j 
idfi= log|D||dti| 
tfidfi,j=tfi,jidfi 
tfi,j：对于文档j，词项i的重要程度（词频）。 
ni,j：词项i出现在文档j里的数量。 
∑knk,j：文档j里总词数。 
idf：词项i的通用价值。 
|D|：语料库里文档的总数。 
|dti|：出现词项i的文档总数。

2、PMI

  点互信息（Pointwise mutual information，PMI），它经常被用于度量两个具体事件的相关程度，公式为：两个词条的PMI公式为： 
  常用的计算PMI(word1, word2)方法是分别 
PMI(x;y)=logp(x,y)p(x)p(y)=logp(x|y)p(x)=logp(y|x)p(y) 
以”word1”，”word2”和”word1NEAR word2”为query，根据搜索引擎检索结果，得到P(word)和P(word1, word2)，如下： 
P(word) = hits(word)N 
P(word1,word2) = hits(word1 NEAR word2)N2
五、特征提取和特征选择

  上面的语言学处理和数学处理，得到的结果，包括词性、词频，TFIDF分数，等等，都可以作为一篇文章的特征使用，有的文本处理方法，比如说向量空间模型，直接使用TFIDF作为特征。通常，针对不同的任务，可以使用不同的特征，比如情感分析，会附带上情感词典（比如正面-情感词的百分比、负面-情感词的百分比、每个句子中否定词个数、每个句子中程度副词个数、每个句子特殊标点符号个数），作为新的特征。序列标注模型，会使用序列边界与否，作为新的特征。 
  获取新的特征后，可以使用传统机器学习的降维方法，对数据进行筛选和降维，比如主成分分析算法（PCA）、局部线性嵌入（LLE）、LDA、Laplacian Eigenmaps 拉普拉斯特征映射等。

六、利用算法进行挖掘

  经过上面的步骤之后，我们就可以把文本集转化成一个矩阵。我们能够利用各种算法进行挖掘，比如说如果要对文本集进行分类，我们可以利用 KNN算法，贝叶斯算法、决策树算法等等。这里，我们就可以利用机器学习的成果。 
  特别一提，就是，自然语言处理里有一个独特的东西，叫做”序列标注”，这个概念，可以结合算法，比如隐马尔可夫、条件随机场等等。序列标注问题应该说是自然语言处理中最常见的问题，而且很可能是最而没有之一。在深度学习没有广泛渗透到各个应用领域之前，传统的最常用的解决序列标注问题的方案是最大熵、CRF等模型，尤其是CRF，基本是最主流的方法。随着深度学习的不断探索和发展，很可能RNN模型会取代CRF的传统霸主地位，会成为解决序列标注问题的标配解决方案。
